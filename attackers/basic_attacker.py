from victim_model import VictimModel
import numpy as np

class BasicAttacker(object):
    """
    A basic attacker that simply returns the input as the output.
    This is a placeholder for more complex attack logic.
    """

    def __init__(self, model: VictimModel, norm_order: int = np.inf, targeted: bool = False, max_query_count: int = 10000, early_stop: bool = False, epsilon_metric: float = 0.01, verbose: bool = True):
        """
        Init the basic attack config.

        Args:
            model (VictimModel): The victim model to be attacked.
            norm_order (int): The attack norm. np.inf or 2.
            targeted (bool): True for targeted attack and False for untargeted attack.
            max_query_count (int): The max query number limit during one attack.
            early_stop (bool): Whether to use the early stopping trick to save the running time.
            epsilon_metric (float): When early_stop == True, epsilon_metric must be provided to control when to early stop an attack.
            verbose (bool): Whether to print the attack log.
        """
        assert (not early_stop) or (epsilon_metric is not None), "Early stopping requires an epsilon metric!"
        self.model = model
        self.norm_order = norm_order
        self.targeted = targeted
        self.max_query_count = max_query_count
        self.early_stop = early_stop
        self.epsilon_metric = epsilon_metric
        self.queries = 0
        self.verbose = verbose

    def query_model_hard_label(self, x):
        """
        Unified interface to query the victim model. This function will increase the inner query counter.

        Args:
            x (torch.Tensor): Samples to be queried.

        Returns:
            y (torch.Tensor): The hard-label output of the victim model on x.
        """
        self.queries += len(x)
        return self.model.query_hard_label(x)

    def is_adversarial(self, x, y, target=None):
        """
        Check whether x is an adversarial sample. This function will increase the inner query counter.

        Args:
            x (torch.Tensor): Sample to be tested.
            y (torch.Tensor): The original label.
            target (torch.Tensor): The target label in an targeted attack. Unused for the untargeted attack.

        Returns:
            whether_adversarial (torch.Tensor(bool)): Whether the input x is an adversarial sample.
        """
        if self.targeted:
            return self.query_model_hard_label(x) == target
        else:
            return self.query_model_hard_label(x) != y

    def attack(self, x, y, target_x=None, target_y=None):
        """
        Perform the attack procedure on the input data.

        Args:
            x (torch.Tensor): Input data. Each pixel value of x should be in the range of [0, 1].
            y (torch.Tensor): True labels for the input data.
            target_x (torch.Tensor, optional): Target data. Defaults to None.
            target_y (torch.Tensor, optional): Labels of target_x_batch. Defaults to None.

        Returns:
            adv_x (torch.Tensor): Adversarial examples generated by the attack.
            
            dist (float): Distances between the original and adversarial examples.

            hist (list[(int, float)]): Attack history. Each element is a tuple of (query_count, dist).
        """
        return x.clone(), np.inf, [(0, np.inf)]